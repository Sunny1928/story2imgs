{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Story 2 images\n",
    "\n",
    "##### 1. find characters traits \n",
    "        1. use NER to find the main characters first appear \n",
    "        2. use SUMMARY to summary sentences between character +-$long \n",
    "        3. use VICUNA to generate prompts about characters' physical look\n",
    "\n",
    "##### 2. highlights (!more about the verbs and the face emotions)\n",
    "        1. find highligh sentence and check the character(s) who would on the picture\n",
    "                may by coference resolution to check the pronun is who \n",
    "        2. find the verb or (face emotions)\n",
    "        3. generate prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModel\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from fastcoref import FCoref\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('story/story1.txt')\n",
    "text = f.read()\n",
    "f.close\n",
    "\n",
    "story = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffHi guys, my name is Tina',\n",
       " ' \\nAll my life I have been shy and have struggled with self-esteem',\n",
       " ' \\nThe kids at school teased me and were cruel to me',\n",
       " '\\n[I was 14 when the most popular girl at school, Jessica, poured juice on my head',\n",
       " ']\\nEveryone laughed as I cried in the bathroom, ashamed',\n",
       " '\\nWhen I returned to the classroom, I saw a new student',\n",
       " ' \\nShe approached me and told me her name was Mary',\n",
       " '\\nWhen I told her what happened to me that day at school, Mary frowned',\n",
       " ' \\nShe said I should take revenge',\n",
       " \" \\n[Although I had doubts, Mary put suspicious pills in Jessica's backpack the next day and called the police\",\n",
       " ']\\nJessica was in shock when two police officers found the pills in her bag',\n",
       " ' \\nI was a little sorry for Jessica, but at the same time, I was happy that she realized what it was like to be humiliated',\n",
       " '\\nI spent more and more time with Mary',\n",
       " ' \\nWhile I was still shy, my best friend was wild and free-spirited',\n",
       " ' \\nOne evening we saw a handsome boy at a party and we both had a crush on him',\n",
       " ' \\nWhen the boy asked if I wanted to kiss him, I blushed and ran away',\n",
       " ' \\nAs I was running away, I turned my head and saw Mary going into the room with him',\n",
       " \" \\nI didn't think much of it, though, so I went to the couch, leaned back on it, and fell asleep\",\n",
       " ' \\nWhen I woke up in the morning, the boy Mary ended up with approached me and kissed me',\n",
       " ' \\nI was shocked',\n",
       " ' \\nI decided to tell Mary about this',\n",
       " \" \\nShe was pissed, telling me not to worry and it wasn't my fault\",\n",
       " ' \\nThe next day, I heard the news that the boy who kissed me at the party was in the hospital',\n",
       " ' \\n[Supposedly, someone pushed him as he went down the steps to the subway',\n",
       " ']\\nI knew immediately that it was Mary',\n",
       " ' \\nWhen I asked her why she did such a thing, she told me to calm down',\n",
       " ' \\nAnd smiling, she said the boy got what he deserved',\n",
       " ' \\nAfter a few days, I was in the locker room',\n",
       " ' \\nI took my clothes off and took a shower',\n",
       " ' \\nSuddenly, someone turned off the light',\n",
       " ' \\nI knew it was one of the mean girls doing a prank on me, but I had to do something about it',\n",
       " ' \\nI peeked into the hallway, holding a towel, and when I was sure there was no one there, I went to the fuse',\n",
       " ' \\nI turned on the light, but when I tried to open the locker room door, I realized it was locked',\n",
       " ' \\n[The school bell rang and suddenly all the kids in the school were standing in the hallway laughing at me',\n",
       " ']\\nI felt anger, standing naked and ashamed',\n",
       " ' \\nI ran outside and saw Mary',\n",
       " ' \\nShe turned into a devil when I told her what had happened',\n",
       " \" \\nI begged her not to do anything, but Mary didn't listen to me\",\n",
       " ' \\n[She opened her backpack, took out her books, and lit them with a match',\n",
       " ']\\n[Within a minute, the fire started spreading through the school',\n",
       " ']\\nFortunately, firefighters arrived quickly and managed to put out the fire',\n",
       " ' \\nOne of the witnesses said he saw me standing outside the school',\n",
       " ' \\nWhen the inspectors asked me what I knew about the accident, I told them it was Mary',\n",
       " \" \\nI didn't know where Mary lived, so I gave the officer her description\",\n",
       " ' \\nAfter a few days, the police called me and my mom to come to the police station',\n",
       " '\\nThey found a recording from a camera across from the school and played it',\n",
       " ' \\nI was shocked when I saw myself on the tape taking books out of my backpack, burning them, and throwing them through the school door',\n",
       " ' \\nWhat the hell? \\nI was speechless when a psychologist told me that I was suffering from a multiple personality disorder',\n",
       " ' \\n[She told me that I created a character that was the complete opposite of me and that I called her Mary',\n",
       " '] \\nSo, Mary never even existed except in my head',\n",
       " ' \\nI realized that it was me who accused Jessica of using and pushing the boy down the stairs',\n",
       " '']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story_lines = story.split('.')\n",
    "\n",
    "story_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I was 14 when the most popular girl at school, Jessica, poured juice on my head.',\n",
       " \"Although I had doubts, Mary put suspicious pills in Jessica's backpack the next day and called the police.\",\n",
       " 'Supposedly, someone pushed him as he went down the steps to the subway.',\n",
       " 'The school bell rang and suddenly all the kids in the school were standing in the hallway laughing at me.',\n",
       " 'She opened her backpack, took out her books, and lit them with a match.',\n",
       " 'Within a minute, the fire started spreading through the school.',\n",
       " 'She told me that I created a character that was the complete opposite of me and that I called her Mary.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "s = re.split('\\[|\\]', story) \n",
    "highlights = []\n",
    "for i in range(len(s)):\n",
    "    if i%2 == 1:\n",
    "        highlights.append(s[i])\n",
    "\n",
    "highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145, 226, 491, 598, 1527, 1599, 2190, 2296, 2494, 2566, 2568, 2632, 3348, 3452)\n"
     ]
    }
   ],
   "source": [
    "matches = [(m.group(0), (m.start())) for m in re.finditer(r'\\[|\\]+', story)]\n",
    "b, match_index = zip(*matches)\n",
    "print(match_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coference Resolution\n",
    "To use the larger but more accurate LingMess model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunny/mambaforge/envs/env_tf/lib/python3.9/site-packages/spacy/util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.5.0) was trained with spaCy v3.5 and may not be 100% compatible with the current version (3.3.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "06/27/2023 00:17:55 - INFO - \t missing_keys: []\n",
      "06/27/2023 00:17:55 - INFO - \t unexpected_keys: []\n",
      "06/27/2023 00:17:55 - INFO - \t mismatched_keys: []\n",
      "06/27/2023 00:17:55 - INFO - \t error_msgs: []\n",
      "06/27/2023 00:17:55 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "06/27/2023 00:17:55 - INFO - \t Tokenize 1 inputs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc3caf8763044c288e2db9dcb7df2940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/27/2023 00:17:55 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524e081781c74c1c92c565c145e94cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = FCoref()\n",
    "preds = model.predict(\n",
    "    texts=[story]\n",
    ")\n",
    "\n",
    "pronun_position = preds[0].get_clusters(as_strings=False)\n",
    "pronun = preds[0].get_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'my', 'I', 'me', 'me', 'I', 'my', 'I', 'I', 'I', 'me', 'me', 'I', 'me', 'I', 'I', 'I', 'I', 'I', 'I', 'my', 'I', 'I', 'I', 'I', 'my', 'I', 'I', 'I', 'me', 'me', 'I', 'I', 'me', 'my', 'I', 'me', 'I', 'I', 'me', 'I', 'I', 'my', 'I', 'me', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'me', 'I', 'I', 'I', 'I', 'me', 'me', 'me', 'I', 'I', 'I', 'I', 'me', 'my', 'I', 'I', 'myself', 'my', 'I', 'me', 'I', 'me', 'I', 'me', 'I', 'my', 'I', 'me'] [(10, 12), (32, 34), (40, 41), (120, 122), (141, 143), (146, 147), (218, 220), (248, 249), (287, 288), (316, 317), (353, 355), (365, 367), (392, 393), (420, 422), (467, 468), (501, 502), (675, 676), (731, 732), (796, 797), (841, 842), (858, 860), (1002, 1003), (1024, 1025), (1052, 1053), (1072, 1073), (1081, 1083), (1133, 1134), (1171, 1172), (1234, 1235), (1298, 1300), (1312, 1314), (1317, 1318), (1333, 1334), (1393, 1395), (1423, 1425), (1448, 1449), (1489, 1491), (1601, 1602), (1644, 1645), (1691, 1693), (1780, 1781), (1807, 1808), (1814, 1816), (1890, 1891), (1943, 1945), (1951, 1952), (1984, 1985), (2037, 2038), (2072, 2073), (2093, 2094), (2125, 2126), (2163, 2164), (2293, 2295), (2298, 2299), (2341, 2342), (2399, 2400), (2430, 2431), (2489, 2491), (2743, 2745), (2802, 2804), (2810, 2811), (2837, 2838), (2863, 2864), (2898, 2899), (2971, 2973), (2978, 2980), (3092, 3093), (3111, 3112), (3117, 3123), (3156, 3158), (3243, 3244), (3285, 3287), (3293, 3294), (3358, 3360), (3366, 3367), (3422, 3424), (3434, 3435), (3493, 3495), (3503, 3504), (3526, 3528)]\n",
      "['a new student', 'She', 'her', 'her', 'Mary', 'She', 'Mary', 'Mary', 'my best friend', 'Mary', 'Mary', 'Mary', 'She', 'Mary', 'her', 'she', 'she', 'she', 'Mary', 'She', 'her', 'her', 'Mary', 'She', 'her', 'her', 'Mary', 'Mary', 'her', 'Mary'] [(322, 335), (338, 341), (368, 371), (399, 402), (443, 447), (458, 461), (515, 519), (828, 832), (858, 872), (1097, 1101), (1268, 1272), (1351, 1355), (1369, 1372), (1632, 1636), (1652, 1655), (1660, 1663), (1682, 1685), (1722, 1725), (2363, 2367), (2370, 2373), (2406, 2409), (2439, 2442), (2467, 2471), (2495, 2498), (2506, 2509), (2529, 2532), (2856, 2860), (2883, 2887), (2917, 2920), (3459, 3463)]\n",
      "['the most popular girl at school, Jessica', \"Jessica's\", 'Jessica', 'her', 'Jessica', 'she', 'Jessica'] [(160, 200), (544, 553), (600, 607), (665, 668), (700, 707), (748, 751), (3541, 3548)]\n",
      "['suspicious pills', 'the pills'] [(524, 540), (652, 661)]\n",
      "[\"Jessica's backpack\", 'her bag'] [(544, 562), (665, 672)]\n",
      "['we', 'we both'] [(914, 916), (951, 958)]\n",
      "['a handsome boy', 'him', 'the boy', 'him', 'him', 'the boy Mary ended up with', 'the boy who kissed me at the party', 'him', 'he', 'the boy', 'he', 'the boy'] [(921, 935), (974, 977), (985, 992), (1019, 1022), (1127, 1130), (1260, 1286), (1470, 1504), (1555, 1558), (1562, 1564), (1731, 1738), (1748, 1750), (3570, 3577)]\n",
      "['the couch', 'it'] [(1181, 1190), (1207, 1209)]\n",
      "['kissed', 'this', 'it'] [(1305, 1311), (1362, 1366), (1413, 1415)]\n",
      "['a party', 'the party'] [(939, 946), (1495, 1504)]\n",
      "['it', 'it'] [(1897, 1899), (1979, 1981)]\n",
      "['the light', 'the light'] [(1878, 1887), (2105, 2114)]\n",
      "['the locker room door', 'it'] [(2141, 2161), (2174, 2176)]\n",
      "['school', 'the school', 'the school', 'the school', 'the school'] [(185, 191), (2241, 2251), (2621, 2631), (2763, 2773), (3065, 3075)]\n",
      "['the hallway', 'the hallway'] [(1998, 2009), (2269, 2280)]\n",
      "['her books', 'them'] [(2529, 2538), (2548, 2552)]\n",
      "['the fire', 'the fire'] [(2586, 2594), (2699, 2707)]\n",
      "['One of the witnesses', 'he'] [(2710, 2730), (2736, 2738)]\n",
      "['the inspectors', 'them'] [(2781, 2795), (2844, 2848)]\n",
      "['the police', 'They'] [(2953, 2963), (3016, 3020)]\n",
      "['a recording from a camera across from the school', 'it', 'the tape'] [(3027, 3075), (3087, 3089), (3127, 3135)]\n",
      "['her backpack', 'my backpack'] [(2506, 2518), (3156, 3167)]\n",
      "['books', 'them', 'them'] [(3143, 3148), (3177, 3181), (3196, 3200)]\n",
      "['a psychologist', 'She'] [(3265, 3279), (3349, 3352)]\n",
      "['a character that was the complete opposite of me', 'her'] [(3376, 3424), (3443, 3446)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(pronun)):\n",
    "    print(pronun[i], pronun_position[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vicuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"./model_grpc/\")\n",
    "from model_server_pb2 import ( modelRequest , modelResponse , modelName , modelInfo , modelConfig)\n",
    "from model_server_pb2_grpc import *\n",
    "import grpc\n",
    "\n",
    "# connect server\n",
    "channel = grpc.insecure_channel(\"140.127.208.185:50051\")\n",
    "client = sendToModelStub(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 5\n",
    "f = open('story/question.txt')\n",
    "text = f.read()\n",
    "f.close\n",
    "text += f\"Q:{story[match_index[index*2-2]+1:match_index[index*2-1]]}\\nA:\"\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN: 'Tina is shy and has struggled with self-esteem all her life. The most popular girl at school, Jessica, poured juice on Tina's head when she was 14. A new student, Mary, told Tina to take revenge.' generate prompts about Tina's appearance? in only one sentence!\n",
      "ASSISTANT: Tina is a shy, insecure teenager with long, curly hair and glasses.\n",
      "HUMAN: 'Tina is shy and has struggled with self-esteem all her life. The most popular girl at school, Jessica, poured juice on Tina's head when she was 14. A new student, Mary, told Tina to take revenge.' generate prompts about Tina's appearance? in only one sentence!\n",
      "ASSISTANT: Tina is a shy, insecure teenager with long, curly hair and glasses.\n",
      "HUMAN: 'Tina is shy and has struggled with self-esteem all her life. The most popular girl at school, Jessica, poured juice on Tina's head when she was 14. A new student, Mary, told Tina to take revenge.' generate prompts about Jessica's appearance? in only one sentence!\n",
      "ASSISTANT: Jessica is a popular, confident teenager with short, straight hair and a bright smile.\n",
      "HUMAN: 'Tina is shy and has struggled with self-esteem all her life. The most popular girl at school, Jessica, poured juice on Tina's head when she was 14. A new student, Mary, told Tina to take revenge.' generate prompts about Mary's appearance? in only one sentence!\n",
      "ASSISTANT: Mary is a new student with long, curly hair and a mysterious air about her.\n",
      "HUMAN: 'Tina is shy and has struggled with self-esteem all her life. The most popular\n",
      "HUMAN: 'Tina is shy and has struggled with self-esteem all her life. The most popular girl at school, Jessica, poured juice on Tina's head when she was 14. A new student, Mary, told Tina to take revenge.' generate prompts about Tina's appearance? in only one sentence!\n",
      "ASSISTANT: Tina is a shy, insecure teenager with long, curly hair and glasses.\n",
      "HUMAN: 'Tina is shy and has struggled with self-esteem all her life. The most popular girl at school, Jessica, poured juice on Tina's head when she was 14. A new student, Mary, told Tina to take revenge.' generate prompts about Tina's appearance? in only one sentence!\n",
      "ASSISTANT: Tina is a shy, insecure teenager with long, curly hair and glasses.\n",
      "HUMAN: 'Tina is shy and has struggled with self-esteem all her life. The most popular girl at school, Jessica, poured juice on Tina's head when she was 14. A new student, Mary, told Tina to take revenge.' generate prompts about Jessica's appearance? in only one sentence!\n",
      "ASSISTANT: Jessica is a popular, confident teenager with short, straight hair and a bright smile.\n",
      "HUMAN: 'Tina is shy and has struggled with self-esteem all her life. The most popular girl at school, Jessica, poured juice on Tina's head when she was 14. A new student, Mary, told Tina to take revenge.' generate prompts about Mary's appearance? in only one sentence!\n",
      "ASSISTANT: Mary is a new student with long, curly hair and a mysterious air about her.\n",
      "HUMAN: 'Tina is shy and has struggled with self-esteem all her life. The most popular\n",
      "HUMAN: 'Tina is shy and has struggled with self-esteem all her life. The most popular girl at school, Jessica, poured juice on Tina's head when she was 14. A new student, Mary, told Tina to take revenge.' generate prompts about Mary's appearance? in only one sentence!\n",
      "ASSISTANT: Mary is a new student with long, curly hair and a mysterious air about her.\n",
      "HUMAN: 'Tina is shy and has struggled with self-esteem all her life. The most popular girl at school, Jessica, poured juice on Tina's head when she was 14. A new student, Mary, told Tina to take revenge.' generate prompts about Jessica's appearance? in only one sentence!\n",
      "ASSISTANT: Jessica is a popular, confident teenager with short, straight hair and a bright smile.\n",
      "HUMAN: 'Tina is shy and has struggled with self-esteem all her life. The most\n"
     ]
    }
   ],
   "source": [
    "# question about characters\n",
    "\n",
    "user = 12\n",
    "res = []\n",
    "\n",
    "for c in characters:\n",
    "    vcuhig = f\"'{summary}' generate prompts about {c}'s appearance? in only one sentence!\"\n",
    "    request = modelRequest(user = user , modelName = \"vicuna\", prompt = vcuhig)\n",
    "    rel = client.getModelResponse(request)\n",
    "    res.append(rel.response)\n",
    "    print(rel.response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN: 'Tina is shy and has struggled with self-esteem all her life. The most popular girl at school, Jessica, poured juice on Tina's head when she was 14. A new student, Mary, told Tina to take revenge.' generate prompts about Tina's appearance? in only one sentence!\n",
      "ASSISTANT: Tina is a shy, insecure teenager with long, curly hair and glasses.\n",
      "HUMAN: 'Tina is shy and has struggled with self-esteem all her life. The most popular girl at school, Jessica, poured juice on Tina's head when she was 14. A new student, Mary, told Tina to take revenge.' generate prompts about Jessica's appearance? in only one sentence!\n",
      "ASSISTANT: Jessica is a popular, confident teenager with short, straight hair and a bright smile.\n",
      "HUMAN: 'Tina is shy and has struggled with self-esteem all her life. The most popular girl at school, Jessica, poured juice on Tina's head when she was 14. A new student, Mary, told Tina to take revenge.' generate prompts about Mary's appearance? in only one sentence!\n",
      "ASSISTANT: Mary is a new student with long, curly hair and a mysterious air about her.\n",
      "HUMAN: 'Tina is shy and has struggled with self-esteem all her life. The most popular\n"
     ]
    }
   ],
   "source": [
    "ans = res[1]\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN: Q:Hi guys, my name is Tina. All my life I have been shy and have struggled with self-esteem. The kids at school teased me and were cruel to me.\n",
      "A: A girl has long, wavy brown hair, a round face, wears glasses, is petite and slender. She walks on the school hallway and other students are laughing at her\n",
      "\n",
      "Q:I was 14 when the most popular girl at school, Jessica, poured juice on my head. \n",
      "Everyone laughed as I cried in the bathroom, ashamed.\n",
      "A: A confident and attractive teenage girl with long, blonde hair, stylish clothing, and a bright smile poured juice\n",
      "\n",
      "Q:When I returned to the classroom, I saw a new student. \n",
      "She approached me and told me her name was Mary.\n",
      "A: A student with short, curly red hair, bright green eyes, and a warm smile introduces herself and extends a friendly handshake.\n",
      "\n",
      "Q:When I told her what happened to me that day at school, Mary frowned. \n",
      "She said I should take revenge.\n",
      "A: A student with short, curly red hair, bright green eyes, and a warm smile with a sympathetic look.\n",
      "\n",
      "Q:Although I had doubts, Mary put suspicious pills in Jessica's backpack the next day and called the police.\n",
      "A: A student with short, curly red hair, bright green eyes, and a warm smile with a look of determination on her face, put pills in someone’s backpack\n",
      "\n",
      "Q:Jessica was in shock when two police officers found the pills in her bag.\n",
      "A: A police officer with a serious expression and  holds up the pills.; a girl looks shocked and nervous. \n",
      "\n",
      "Q:I was a little sorry for Jessica, but at the same time, I was happy that she realized what it was like to be humiliated.\n",
      "A: A girl with long, wavy brown hair, a round face, wearing glasses, and petite and slender body, with a sympathetic expression on her face.\n",
      "\n",
      "Q:One evening we saw a handsome boy at a party and we both had a crush on him.\n",
      "A: a handsome boy at a party\n",
      "Q:She opened her backpack, took out her books, and lit them with a match.\n",
      "A:\n",
      "ASSISTANT: A girl with long, wavy brown hair, a round face, wearing glasses, and petite and slender body, with a sympathetic expression on her face, opens her backpack, takes out her books, and lights them with a match.\n"
     ]
    }
   ],
   "source": [
    "# question about highlight\n",
    "\n",
    "user = 12\n",
    "request = modelRequest(user = user , modelName = \"vicuna\", prompt = text)\n",
    "rel = client.getModelResponse(request)\n",
    "print(rel.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "\n",
    "ner_model = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tina', 'Jessica', 'Mary']\n",
      "[{'entity': 'B-PER', 'score': 0.9292482, 'index': 8, 'word': 'Tina', 'start': 21, 'end': 25}, {'entity': 'B-PER', 'score': 0.984562, 'index': 52, 'word': 'Jessica', 'start': 193, 'end': 200}, {'entity': 'B-PER', 'score': 0.9823167, 'index': 103, 'word': 'Mary', 'start': 381, 'end': 385}]\n"
     ]
    }
   ],
   "source": [
    "ner_results = ner_model(story)\n",
    "\n",
    "characters_info = []\n",
    "characters = []\n",
    "\n",
    "for enity in ner_results:\n",
    "    # print(enity)\n",
    "\n",
    "    if enity['word'] not in characters:    \n",
    "        characters.append(enity['word'])\n",
    "        characters_info.append(enity)\n",
    "    last_index = enity['index']\n",
    "\n",
    "print(characters)\n",
    "print(characters_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMMARIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"philschmid/bart-large-cnn-samsum\")\n",
    "\n",
    "# summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "# print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tina has been bullied at school since she was 14. She had a crush on a boy at a party. Mary put suspicious pills in Jessica's backpack the next day and called the police. The next day, the boy who kissed Tina at the party was taken to hospital. Tina and her mother went to the police station. A recording from a camera across from the school was played to them. Tina took the books out of her backpack and set the school on fire.\n",
      "85\n"
     ]
    }
   ],
   "source": [
    "# summary the whole story\n",
    "\n",
    "summary = summarizer(story, max_length=250, min_length=30, do_sample=False)[0]['summary_text']\n",
    "print(summary)\n",
    "print(len(summary.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary found suspicious pills in Jessica's backpack the next day and called the police. She had doubts, but Mary had already put them there.\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# summary highlight\n",
    "\n",
    "summary = summarizer(highlights[1], max_length=250, min_length=30, do_sample=False)[0]['summary_text']\n",
    "print(summary)\n",
    "print(len(summary.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tina is shy and has struggled with self-esteem all her life. The kids at school teased her and were cruel to her.\n",
      "22\n",
      "Tina is shy and has struggled with self-esteem all her life. She was teased at school when she was 14 and Jessica poured juice on her head.\n",
      "27\n",
      "Tina is shy and has struggled with self-esteem all her life. The most popular girl at school, Jessica, poured juice on Tina's head when she was 14. A new student, Mary, told Tina to take revenge.\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "# summary every character's trate\n",
    "long = 120\n",
    "\n",
    "for u in range(len(characters)):\n",
    "    # start = characters_info[u]['start']-long if characters_info[u]['end']-long >=0 else 0\n",
    "    start = 0\n",
    "    end = characters_info[u]['end']+long\n",
    "    # print(start)\n",
    "    # print(end)\n",
    "    summary = summarizer(story[start: end], max_length=250, min_length=30, do_sample=False)[0]['summary_text']\n",
    "    print(summary)\n",
    "    print(len(summary.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary by piceses\n",
    "\n",
    "length = 2000\n",
    "times = int(len(story)/length)\n",
    "sums = ''\n",
    "\n",
    "for i in range(times+1):\n",
    "    sum_results = summarizer(story[length*i:length*(i+1)])\n",
    "    sums = sums + sum_results[0]['summary_text'] + '\\n'\n",
    "\n",
    "sum_results = summarizer(story[length*(times+1):])\n",
    "sums = sums + sum_results[0]['summary_text'] + '\\n'\n",
    "print(sums)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA\n",
    "1. it's hard to using qa to ask how does character look or wear or even using point to filter it in the whole story. \n",
    "2. summary and qa together not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "qa_model = pipeline('question-answering', model=model_name, tokenizer=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how is Tina look\n",
      "{'score': 0.12119999527931213, 'start': 52, 'end': 55, 'answer': 'shy'}\n",
      "how is Jessica look\n",
      "{'score': 0.0673554390668869, 'start': 202, 'end': 225, 'answer': 'poured juice on my head'}\n",
      "how is Mary look\n",
      "{'score': 0.2998819351196289, 'start': 448, 'end': 455, 'answer': 'frowned'}\n"
     ]
    }
   ],
   "source": [
    "for u in range(len(characters)):\n",
    "    QA_input = {\n",
    "        # 'question': f'who is {characters[u]}',\n",
    "        # 'question': f'how does {characters[u]} look',\n",
    "        'question': f'how is {characters[u]} look',\n",
    "        'context': story[:characters_info[u]['end']+90]\n",
    "    }\n",
    "    res = qa_model(QA_input)\n",
    "    print(QA_input['question'])\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how is Tina\n",
      "{'score': 0.03173093870282173, 'start': 1, 'end': 25, 'answer': 'Hi guys, my name is Tina'}\n",
      "how is Jessica\n",
      "{'score': 0.04063982889056206, 'start': 202, 'end': 225, 'answer': 'poured juice on my head'}\n",
      "how is Mary\n",
      "{'score': 0.26601850986480713, 'start': 2370, 'end': 2393, 'answer': 'She turned into a devil'}\n"
     ]
    }
   ],
   "source": [
    "# the whole story in every character\n",
    "\n",
    "for u in range(len(characters)):\n",
    "    QA_input = {\n",
    "        'question': f'how is {characters[u]}',\n",
    "        'context': story\n",
    "    }\n",
    "    res = qa_model(QA_input)\n",
    "    print(QA_input['question'])\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tina is shy and has struggled with self-esteem all her life. The kids at school teased her and were cruel to her.\n",
      "who is Tina\n",
      "{'score': 0.7117064595222473, 'start': 8, 'end': 11, 'answer': 'shy'}\n",
      "Tina is shy and has struggled with self-esteem all her life. She was teased at school when she was 14 and Jessica poured juice on her head.\n",
      "who is Jessica\n",
      "{'score': 0.0002758566115517169, 'start': 106, 'end': 138, 'answer': 'Jessica poured juice on her head'}\n",
      "Tina is shy and has struggled with self-esteem all her life. The most popular girl at school, Jessica, poured juice on Tina's head when she was 14. A new student, Mary, told Tina to take revenge.\n",
      "who is Mary\n",
      "{'score': 0.5061036944389343, 'start': 148, 'end': 161, 'answer': 'A new student'}\n"
     ]
    }
   ],
   "source": [
    "# summary every character's trate and then do qa\n",
    "long = 120\n",
    "\n",
    "for u in range(len(characters)):\n",
    "    # start = characters_info[u]['start']-long if characters_info[u]['end']-long >=0 else 0\n",
    "    start = 0\n",
    "    end = characters_info[u]['end']+long\n",
    "    # print(start)\n",
    "    # print(end)\n",
    "    summary = summarizer(story[start: end], max_length=250, min_length=30, do_sample=False)[0]['summary_text']\n",
    "    print(summary)\n",
    "    # print(len(summary.split()))\n",
    "\n",
    "    QA_input = {\n",
    "        'question': f'who is {characters[u]}',\n",
    "        # 'question': f'how does {characters[u]} look',\n",
    "        # 'question': f'how is {characters[u]} look',\n",
    "        'context': summary\n",
    "    }\n",
    "    res = qa_model(QA_input)\n",
    "    print(QA_input['question'])\n",
    "    print(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find subjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc=nlp(text)\n",
    "\n",
    "sub_toks = [tok for tok in doc if (tok.dep_ == \"nsubj\") ]\n",
    "\n",
    "print(sub_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(text)\n",
    "# print([w.text for w in doc if (w.pos_ == \"PROPN\")])\n",
    "print(len(doc))\n",
    "for i in range(len(doc)):\n",
    "    if doc[i].pos_ == \"PROPN\":\n",
    "        print(doc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import DefaultTagger, UnigramTagger, BigramTagger, TrigramTagger\n",
    "\n",
    "class SubjectTrigramTagger(object):\n",
    "\n",
    "    \"\"\" Creates an instance of NLTKs TrigramTagger with a backoff\n",
    "    tagger of a bigram tagger a unigram tagger and a default tagger that sets\n",
    "    all words to nouns (NN)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_sents):\n",
    "\n",
    "        \"\"\"\n",
    "        train_sents: trained sentences which have already been tagged.\n",
    "                Currently using Brown, conll2000, and TreeBank corpuses\n",
    "        \"\"\"\n",
    "\n",
    "        t0 = DefaultTagger('NN')\n",
    "        t1 = UnigramTagger(train_sents, backoff=t0)\n",
    "        t2 = BigramTagger(train_sents, backoff=t1)\n",
    "        self.tagger = TrigramTagger(train_sents, backoff=t2)\n",
    "\n",
    "    def tag(self, tokens):\n",
    "        return self.tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pickle\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Noun Part of Speech Tags used by NLTK\n",
    "# More can be found here\n",
    "# http://www.winwaed.com/blog/2011/11/08/part-of-speech-tags/\n",
    "NOUNS = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "VERBS = ['VB', 'VBG', 'VBD', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "def clean_document(document):\n",
    "    \"\"\"Remove enronious characters. Extra whitespace and stop words\"\"\"\n",
    "    document = re.sub('[^A-Za-z .-]+', ' ', document)\n",
    "    document = ' '.join(document.split())\n",
    "    document = ' '.join([i for i in document.split() if i not in stop])\n",
    "    return document\n",
    "\n",
    "def tokenize_sentences(document):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    return sentences\n",
    "\n",
    "def get_entities(document):\n",
    "    \"\"\"Returns Named Entities using NLTK Chunking\"\"\"\n",
    "    entities = []\n",
    "    sentences = tokenize_sentences(document)\n",
    "\n",
    "    # Part of Speech Tagging\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    for tagged_sentence in sentences:\n",
    "        for chunk in nltk.ne_chunk(tagged_sentence):\n",
    "            if type(chunk) == nltk.tree.Tree:\n",
    "                entities.append(' '.join([c[0] for c in chunk]).lower())\n",
    "    return entities\n",
    "\n",
    "def word_freq_dist(document):\n",
    "    \"\"\"Returns a word count frequency distribution\"\"\"\n",
    "    words = nltk.tokenize.word_tokenize(document)\n",
    "    words = [word.lower() for word in words if word not in stop]\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    return fdist\n",
    "\n",
    "def extract_subject(document):\n",
    "    # Get most frequent Nouns\n",
    "    fdist = word_freq_dist(document)\n",
    "    most_freq_nouns = [w for w, c in fdist.most_common(100)\n",
    "                       if nltk.pos_tag([w])[0][1] in NOUNS]\n",
    "    print(most_freq_nouns)\n",
    "    \n",
    "\n",
    "    # Get Top 10 entities\n",
    "    entities = get_entities(document)\n",
    "    top_10_entities = [w for w, c in nltk.FreqDist(entities).most_common(10)]\n",
    "\n",
    "    # Get the subject noun by looking at the intersection of top 10 entities\n",
    "    # and most frequent nouns. It takes the first element in the list\n",
    "    subject_nouns = [entity for entity in top_10_entities\n",
    "                    if entity.split()[0] in most_freq_nouns]\n",
    "    print(subject_nouns)\n",
    "    return subject_nouns[0]\n",
    "\n",
    "def trained_tagger(existing=False):\n",
    "    \"\"\"Returns a trained trigram tagger\n",
    "\n",
    "    existing : set to True if already trained tagger has been pickled\n",
    "    \"\"\"\n",
    "    if existing:\n",
    "        trigram_tagger = pickle.load(open('trained_tagger.pkl', 'rb'))\n",
    "        return trigram_tagger\n",
    "\n",
    "    # Aggregate trained sentences for N-Gram Taggers\n",
    "    train_sents = nltk.corpus.brown.tagged_sents()\n",
    "    train_sents += nltk.corpus.conll2000.tagged_sents()\n",
    "    train_sents += nltk.corpus.treebank.tagged_sents()\n",
    "\n",
    "    # Create instance of SubjectTrigramTagger and persist instance of it\n",
    "    trigram_tagger = SubjectTrigramTagger(train_sents)\n",
    "    pickle.dump(trigram_tagger, open('trained_tagger.pkl', 'wb'))\n",
    "\n",
    "    return trigram_tagger\n",
    "\n",
    "def tag_sentences(subject, document):\n",
    "    \"\"\"Returns tagged sentences using POS tagging\"\"\"\n",
    "    trigram_tagger = trained_tagger(existing=False)\n",
    "\n",
    "    # Tokenize Sentences and words\n",
    "    sentences = tokenize_sentences(document)\n",
    "    merge_multi_word_subject(sentences, subject)\n",
    "\n",
    "    # Filter out sentences where subject is not present\n",
    "    sentences = [sentence for sentence in sentences if subject in\n",
    "                [word.lower() for word in sentence]]\n",
    "\n",
    "    # Tag each sentence\n",
    "    tagged_sents = [trigram_tagger.tag(sent) for sent in sentences]\n",
    "    return tagged_sents\n",
    "\n",
    "def merge_multi_word_subject(sentences, subject):\n",
    "    \"\"\"Merges multi word subjects into one single token\n",
    "    ex. [('steve', 'NN', ('jobs', 'NN')] -> [('steve jobs', 'NN')]\n",
    "    \"\"\"\n",
    "    if len(subject.split()) == 1:\n",
    "        return sentences\n",
    "    subject_lst = subject.split()\n",
    "    sentences_lower = [[word.lower() for word in sentence]\n",
    "                        for sentence in sentences]\n",
    "    for i, sent in enumerate(sentences_lower):\n",
    "        if subject_lst[0] in sent:\n",
    "            for j, token in enumerate(sent):\n",
    "                start = subject_lst[0] == token\n",
    "                exists = subject_lst == sent[j:j+len(subject_lst)]\n",
    "                if start and exists:\n",
    "                    del sentences[i][j+1:j+len(subject_lst)]\n",
    "                    sentences[i][j] = subject\n",
    "    return sentences\n",
    "\n",
    "def get_svo(sentence, subject):\n",
    "    \"\"\"Returns a dictionary containing:\n",
    "\n",
    "    subject : the subject determined earlier\n",
    "    action : the action verb of particular related to the subject\n",
    "    object : the object the action is referring to\n",
    "    phrase : list of token, tag pairs for that lie within the indexes of\n",
    "                the variables above\n",
    "    \"\"\"\n",
    "    subject_idx = next((i for i, v in enumerate(sentence)\n",
    "                    if v[0].lower() == subject), None)\n",
    "    data = {'subject': subject}\n",
    "    for i in range(subject_idx, len(sentence)):\n",
    "        found_action = False\n",
    "        for j, (token, tag) in enumerate(sentence[i+1:]):\n",
    "            if tag in VERBS:\n",
    "                data['action'] = token\n",
    "                found_action = True\n",
    "            if tag in NOUNS and found_action == True:\n",
    "                data['object'] = token\n",
    "                data['phrase'] = sentence[i: i+j+2]\n",
    "                return data\n",
    "    return {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('brown')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    document = clean_document(text)\n",
    "    subject = extract_subject(document)\n",
    "    print(subject)\n",
    "\n",
    "    # tagged_sents = tag_sentences(subject, document)\n",
    "\n",
    "    # svos = [get_svo(sentence, subject)for sentence in tagged_sents]\n",
    "    # for svo in svos:\n",
    "    #     if svo:\n",
    "    #         print(svo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一開始先抓好人不太好！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# a) Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question': 'Why is model conversion important?',\n",
    "    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n",
    "}\n",
    "res = nlp(QA_input)\n",
    "\n",
    "# b) Load model & tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_input = {\n",
    "    'question': 'What characters in the context?',\n",
    "    'context': text\n",
    "}\n",
    "res = nlp(QA_input)\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
